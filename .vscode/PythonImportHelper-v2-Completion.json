[
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Faker",
        "importPath": "faker",
        "description": "faker",
        "isExtraImport": true,
        "detail": "faker",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "Decimal",
        "importPath": "decimal",
        "description": "decimal",
        "isExtraImport": true,
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "init_chat_model",
        "importPath": "langchain.chat_models",
        "description": "langchain.chat_models",
        "isExtraImport": true,
        "detail": "langchain.chat_models",
        "documentation": {}
    },
    {
        "label": "ChatGoogleGenerativeAI",
        "importPath": "langchain_google_genai",
        "description": "langchain_google_genai",
        "isExtraImport": true,
        "detail": "langchain_google_genai",
        "documentation": {}
    },
    {
        "label": "mysql.connector",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mysql.connector",
        "description": "mysql.connector",
        "detail": "mysql.connector",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sqlglot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlglot",
        "description": "sqlglot",
        "detail": "sqlglot",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "sqlparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlparse",
        "description": "sqlparse",
        "detail": "sqlparse",
        "documentation": {}
    },
    {
        "label": "pymysql",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pymysql",
        "description": "pymysql",
        "detail": "pymysql",
        "documentation": {}
    },
    {
        "label": "write_to_json_file",
        "importPath": "utils.file_utils",
        "description": "utils.file_utils",
        "isExtraImport": true,
        "detail": "utils.file_utils",
        "documentation": {}
    },
    {
        "label": "generate_caller_script_with_vars",
        "importPath": "utils.file_utils",
        "description": "utils.file_utils",
        "isExtraImport": true,
        "detail": "utils.file_utils",
        "documentation": {}
    },
    {
        "label": "generate_DimCategory",
        "importPath": "do.DimCategory",
        "description": "do.DimCategory",
        "isExtraImport": true,
        "detail": "do.DimCategory",
        "documentation": {}
    },
    {
        "label": "generate_DimDate",
        "importPath": "do.DimDate",
        "description": "do.DimDate",
        "isExtraImport": true,
        "detail": "do.DimDate",
        "documentation": {}
    },
    {
        "label": "generate_DimPaymentMethod",
        "importPath": "do.DimPaymentMethod",
        "description": "do.DimPaymentMethod",
        "isExtraImport": true,
        "detail": "do.DimPaymentMethod",
        "documentation": {}
    },
    {
        "label": "generate_DimProduct",
        "importPath": "do.DimProduct",
        "description": "do.DimProduct",
        "isExtraImport": true,
        "detail": "do.DimProduct",
        "documentation": {}
    },
    {
        "label": "generate_DimShipping",
        "importPath": "do.DimShipping",
        "description": "do.DimShipping",
        "isExtraImport": true,
        "detail": "do.DimShipping",
        "documentation": {}
    },
    {
        "label": "generate_DimUser",
        "importPath": "do.DimUser",
        "description": "do.DimUser",
        "isExtraImport": true,
        "detail": "do.DimUser",
        "documentation": {}
    },
    {
        "label": "generate_FactOrders",
        "importPath": "do.FactOrders",
        "description": "do.FactOrders",
        "isExtraImport": true,
        "detail": "do.FactOrders",
        "documentation": {}
    },
    {
        "label": "generate_FactPayments",
        "importPath": "do.FactPayments",
        "description": "do.FactPayments",
        "isExtraImport": true,
        "detail": "do.FactPayments",
        "documentation": {}
    },
    {
        "label": "generate_FactShipping",
        "importPath": "do.FactShipping",
        "description": "do.FactShipping",
        "isExtraImport": true,
        "detail": "do.FactShipping",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "get_llm",
        "importPath": "utils.llm_utils",
        "description": "utils.llm_utils",
        "isExtraImport": true,
        "detail": "utils.llm_utils",
        "documentation": {}
    },
    {
        "label": "get_llm",
        "importPath": "utils.llm_utils",
        "description": "utils.llm_utils",
        "isExtraImport": true,
        "detail": "utils.llm_utils",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "insert_sql_data",
        "importPath": "utils.sql_utils",
        "description": "utils.sql_utils",
        "isExtraImport": true,
        "detail": "utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "generate_all_inserts",
        "importPath": "utils.sql_utils",
        "description": "utils.sql_utils",
        "isExtraImport": true,
        "detail": "utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "generate_all_inserts",
        "importPath": "utils.sql_utils",
        "description": "utils.sql_utils",
        "isExtraImport": true,
        "detail": "utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "main_faker",
        "importPath": "call_faker",
        "description": "call_faker",
        "isExtraImport": true,
        "detail": "call_faker",
        "documentation": {}
    },
    {
        "label": "get_table_order",
        "importPath": "ddl_info_extractor",
        "description": "ddl_info_extractor",
        "isExtraImport": true,
        "detail": "ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "get_ddls_for_tables",
        "importPath": "ddl_info_extractor",
        "description": "ddl_info_extractor",
        "isExtraImport": true,
        "detail": "ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "get_table_order",
        "importPath": "ddl_info_extractor",
        "description": "ddl_info_extractor",
        "isExtraImport": true,
        "detail": "ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "get_ddls_for_tables",
        "importPath": "ddl_info_extractor",
        "description": "ddl_info_extractor",
        "isExtraImport": true,
        "detail": "ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "importlib.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib.util",
        "description": "importlib.util",
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "create_insertion_data_methods",
        "importPath": "process_tables",
        "description": "process_tables",
        "isExtraImport": true,
        "detail": "process_tables",
        "documentation": {}
    },
    {
        "label": "code_generator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "code_generator",
        "description": "code_generator",
        "detail": "code_generator",
        "documentation": {}
    },
    {
        "label": "data_generator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "data_generator",
        "description": "data_generator",
        "detail": "data_generator",
        "documentation": {}
    },
    {
        "label": "sql_utils",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "generate_DimCategory",
        "kind": 2,
        "importPath": "db.do.DimCategory",
        "description": "db.do.DimCategory",
        "peekOfCode": "def generate_DimCategory(n=5000, seq_start=1):\n    \"\"\"\n    Generates synthetic data for the DimCategory table.\n    Args:\n        n (int): The number of rows to generate.\n        seq_start (int): The starting value for the primary key sequence.\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a row.\n    \"\"\"\n    fake = Faker()",
        "detail": "db.do.DimCategory",
        "documentation": {}
    },
    {
        "label": "generate_DimDate",
        "kind": 2,
        "importPath": "db.do.DimDate",
        "description": "db.do.DimDate",
        "peekOfCode": "def generate_DimDate(n=5000, seq_start=1):\n    fake = Faker()\n    data = []\n    current_date_key = seq_start\n    # Start date for the dimension table\n    start_date = datetime(2020, 1, 1)\n    for i in range(n):\n        current_date = start_date + timedelta(days=i)\n        # Derive date parts\n        year = current_date.year",
        "detail": "db.do.DimDate",
        "documentation": {}
    },
    {
        "label": "generate_DimPaymentMethod",
        "kind": 2,
        "importPath": "db.do.DimPaymentMethod",
        "description": "db.do.DimPaymentMethod",
        "peekOfCode": "def generate_DimPaymentMethod(n=5000, seq_start=1):\n    fake = Faker()\n    data = []\n    for i in range(n):\n        payment_method_key = seq_start + i\n        method = fake.word()\n        if len(method) > 50:\n            method = method[:50]\n        data.append({\n            \"PaymentMethodKey\": payment_method_key,",
        "detail": "db.do.DimPaymentMethod",
        "documentation": {}
    },
    {
        "label": "generate_DimProduct",
        "kind": 2,
        "importPath": "db.do.DimProduct",
        "description": "db.do.DimProduct",
        "peekOfCode": "def generate_DimProduct(n=5000, seq_start=1):\n    fake = Faker()\n    data = []\n    for i in range(n):\n        product_key = seq_start + i\n        # ProductID: Not unique, generate a random integer\n        product_id = fake.random_int(min=1000, max=99999)\n        # Name: Not unique, generate a product-like name\n        name = fake.catch_phrase()\n        # Ensure length respects VARCHAR(255) - catch_phrase is usually short",
        "detail": "db.do.DimProduct",
        "documentation": {}
    },
    {
        "label": "generate_DimShipping",
        "kind": 2,
        "importPath": "db.do.DimShipping",
        "description": "db.do.DimShipping",
        "peekOfCode": "def generate_DimShipping(n=5000, seq_start=1):\n    \"\"\"\n    Generates synthetic data for the DimShipping table.\n    Args:\n        n (int): The number of rows to generate.\n        seq_start (int): The starting value for the ShippingKey primary key.\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a row\n              in the DimShipping table.\n    \"\"\"",
        "detail": "db.do.DimShipping",
        "documentation": {}
    },
    {
        "label": "generate_DimUser",
        "kind": 2,
        "importPath": "db.do.DimUser",
        "description": "db.do.DimUser",
        "peekOfCode": "def generate_DimUser(n=5000, seq_start=1):\n    fake = Faker()\n    data = []\n    roles = ['Admin', 'User', 'Moderator', 'Guest'] # Example roles\n    for i in range(n):\n        user_key = seq_start + i\n        # UserID: Not unique, generate random int\n        user_id = random.randint(1000, 99999)\n        # Name: Description\n        name = fake.name()",
        "detail": "db.do.DimUser",
        "documentation": {}
    },
    {
        "label": "generate_FactOrders",
        "kind": 2,
        "importPath": "db.do.FactOrders",
        "description": "db.do.FactOrders",
        "peekOfCode": "def generate_FactOrders(DimUser=None, DimProduct=None, DimDate=None, n=5000, seq_start=1):\n    \"\"\"\n    Generates synthetic data for the FactOrders table.\n    Args:\n        DimUser: List of dictionaries from DimUser table (must contain 'UserKey').\n        DimProduct: List of dictionaries from DimProduct table (must contain 'ProductKey').\n        DimDate: List of dictionaries from DimDate table (must contain 'DateKey').\n        n: The number of rows to generate.\n        seq_start: The starting sequence number for the primary key.\n    Returns:",
        "detail": "db.do.FactOrders",
        "documentation": {}
    },
    {
        "label": "generate_FactPayments",
        "kind": 2,
        "importPath": "db.do.FactPayments",
        "description": "db.do.FactPayments",
        "peekOfCode": "def generate_FactPayments(FactOrders=None, DimPaymentMethod=None, DimDate=None, n=5000, seq_start=1):\n    fake = Faker()\n    data = []\n    # Extract potential foreign key values from dependencies\n    order_keys = [order['OrderKey'] for order in FactOrders] if FactOrders else []\n    payment_method_keys = [method['PaymentMethodKey'] for method in DimPaymentMethod] if DimPaymentMethod else []\n    date_keys = [date['DateKey'] for date in DimDate] if DimDate else []\n    # Check if essential foreign key data is available\n    if not order_keys or not payment_method_keys or not date_keys:\n        # Cannot generate meaningful fact data without dimensions/orders",
        "detail": "db.do.FactPayments",
        "documentation": {}
    },
    {
        "label": "generate_FactShipping",
        "kind": 2,
        "importPath": "db.do.FactShipping",
        "description": "db.do.FactShipping",
        "peekOfCode": "def generate_FactShipping(FactOrders=None, DimDate=None, n=5000, seq_start=1):\n    fake = Faker()\n    data = []\n    if not FactOrders or not isinstance(FactOrders, list) or not FactOrders:\n        print(\"Warning: FactOrders data not provided or is empty. Cannot generate FactShipping.\")\n        return []\n    if not DimDate or not isinstance(DimDate, list) or not DimDate:\n        print(\"Warning: DimDate data not provided or is empty. Cannot generate FactShipping.\")\n        return []\n    # Extract OrderKeys from FactOrders",
        "detail": "db.do.FactShipping",
        "documentation": {}
    },
    {
        "label": "topological_sort",
        "kind": 2,
        "importPath": "db.utils.file_utils",
        "description": "db.utils.file_utils",
        "peekOfCode": "def topological_sort(dependency_graph):\n    from collections import defaultdict, deque\n    indegree = defaultdict(int)\n    graph = defaultdict(list)\n    for node, deps in dependency_graph.items():\n        indegree[node] = indegree.get(node, 0)\n        for dep in deps:\n            graph[dep].append(node)\n            indegree[node] += 1\n    queue = deque([node for node in indegree if indegree[node] == 0])",
        "detail": "db.utils.file_utils",
        "documentation": {}
    },
    {
        "label": "generate_caller_script_with_vars",
        "kind": 2,
        "importPath": "db.utils.file_utils",
        "description": "db.utils.file_utils",
        "peekOfCode": "def generate_caller_script_with_vars(dependency_graph,sorted_tables, output_path=\"call_faker.py\", module_folder=\"do\"):\n    # sorted_tables = topological_sort(dependency_graph)\n    print(f\"sorted tables : {sorted_tables}\")\n    with open(output_path, \"w\") as f:\n        f.write(\"# Auto-generated script to call generate functions in dependency order\\n\\n\")\n        f.write(\"import os\\n\")\n        f.write(\"import json\\n\")\n        f.write(\"from decimal import Decimal\\n\\n\")\n        f.write(\"from utils.file_utils import write_to_json_file\\n\\n\")\n        # Write imports",
        "detail": "db.utils.file_utils",
        "documentation": {}
    },
    {
        "label": "write_to_json_file",
        "kind": 2,
        "importPath": "db.utils.file_utils",
        "description": "db.utils.file_utils",
        "peekOfCode": "def write_to_json_file(var_name, data):\n    base_temp_dir = tempfile.gettempdir()  # e.g., /var/folders/... on macOS\n    full_output_dir = os.path.join(base_temp_dir, \"output\")\n    os.makedirs(full_output_dir, exist_ok=True)\n    file_path = os.path.join(full_output_dir, f\"{var_name}.json\")\n    # Delete if file already exists\n    if os.path.exists(file_path):\n        os.remove(file_path)\n    def default_serializer(obj):\n        if isinstance(obj, Decimal):",
        "detail": "db.utils.file_utils",
        "documentation": {}
    },
    {
        "label": "get_llm",
        "kind": 2,
        "importPath": "db.utils.llm_utils",
        "description": "db.utils.llm_utils",
        "peekOfCode": "def get_llm():\n    load_dotenv()\n    model = os.getenv(\"LLM_MODEL\")\n    model_provider = os.getenv(\"LLM_MODEL_PROVIDER\")\n    print(f\"LLM_MODEL={model} and LLM_MODEL_PROVIDER={model_provider}\")\n    if model_provider == \"google_genai\":\n        return ChatGoogleGenerativeAI(\n            model=model,\n            google_api_key=os.getenv(\"GEMINI_API_KEY\")\n        )",
        "detail": "db.utils.llm_utils",
        "documentation": {}
    },
    {
        "label": "execute_sql_file",
        "kind": 2,
        "importPath": "db.utils.sql_utils",
        "description": "db.utils.sql_utils",
        "peekOfCode": "def execute_sql_file(sql_file_path):\n    \"\"\"\n    Executes all SQL statements from the given .sql file on a MySQL database.\n    Continues execution even if some statements fail.\n    Args:\n        sql_file_path (str): Path to the SQL file.\n    \"\"\"\n    db_config = {\n        \"host\": os.getenv('DB_HOST'),\n        \"port\": os.getenv(\"DB_PORT\"),",
        "detail": "db.utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "generate_all_inserts",
        "kind": 2,
        "importPath": "db.utils.sql_utils",
        "description": "db.utils.sql_utils",
        "peekOfCode": "def generate_all_inserts(sorted_tables, ddls, json_dir):\n    # Generate a timestamped directory name\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    base_temp_dir = tempfile.gettempdir()\n    full_inserts_dir = os.path.join(base_temp_dir, \"inserts\")\n    # Full path with timestamped subfolder\n    inserts_dir = os.path.join(full_inserts_dir, f\"inserts_{timestamp}\")\n    # Ensure the parent inserts folder exists (optional)\n    os.makedirs(\"inserts\", exist_ok=True)\n    # Create the timestamped inserts directory",
        "detail": "db.utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "clean_values_sql",
        "kind": 2,
        "importPath": "db.utils.sql_utils",
        "description": "db.utils.sql_utils",
        "peekOfCode": "def clean_values_sql(values_sql: str, columns: list[str]) -> str:\n    # Use csv.reader to safely parse values with commas inside quotes\n    reader = csv.reader(io.StringIO(values_sql), skipinitialspace=True, quotechar=\"'\")\n    raw_values = next(reader)\n    # Trim trailing NULLs until we match the number of columns\n    column_count = len(columns)\n    print(f\"The columns are : {columns} and count is {column_count}\")\n    while len(raw_values) > column_count and raw_values[-1].strip().upper() == 'NULL':\n        raw_values.pop()\n    # Reformat values: keep numbers and NULL unquoted, escape strings",
        "detail": "db.utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "get_db_connection",
        "kind": 2,
        "importPath": "db.utils.sql_utils",
        "description": "db.utils.sql_utils",
        "peekOfCode": "def get_db_connection():\n    return pymysql.connect(\n        host=os.getenv(\"DB_HOST\"),\n        user=os.getenv(\"DB_USER\"),\n        password=os.getenv(\"DB_PASSWORD\"),\n        database=os.getenv(\"DB_NAME\"),\n        port=int(os.getenv(\"DB_PORT\", 3306))\n    )\ndef log_write(log_path, message):\n    with open(log_path, \"a\") as f:",
        "detail": "db.utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "log_write",
        "kind": 2,
        "importPath": "db.utils.sql_utils",
        "description": "db.utils.sql_utils",
        "peekOfCode": "def log_write(log_path, message):\n    with open(log_path, \"a\") as f:\n        f.write(message + \"\\n\")\ndef insert_sql_data(sorted_tables, inserts_dir):\n    logs_dir = os.path.join(inserts_dir, \"logs\")\n    os.makedirs(logs_dir, exist_ok=True)\n    log_file = os.path.join(logs_dir, \"insert_log.txt\")\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    # if any(isinstance(t, list) for t in sorted_tables):",
        "detail": "db.utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "insert_sql_data",
        "kind": 2,
        "importPath": "db.utils.sql_utils",
        "description": "db.utils.sql_utils",
        "peekOfCode": "def insert_sql_data(sorted_tables, inserts_dir):\n    logs_dir = os.path.join(inserts_dir, \"logs\")\n    os.makedirs(logs_dir, exist_ok=True)\n    log_file = os.path.join(logs_dir, \"insert_log.txt\")\n    conn = get_db_connection()\n    cursor = conn.cursor()\n    # if any(isinstance(t, list) for t in sorted_tables):\n    #     sorted_tables = [item for sublist in sorted_tables for item in sublist]\n    sorted_tables = sorted_tables[0]\n    for table in sorted_tables:",
        "detail": "db.utils.sql_utils",
        "documentation": {}
    },
    {
        "label": "main_faker",
        "kind": 2,
        "importPath": "db.call_faker",
        "description": "db.call_faker",
        "peekOfCode": "def main_faker():\n    print('Calling generate_DimCategory()...')\n    DimCategory = generate_DimCategory()\n    write_to_json_file('DimCategory', DimCategory)\n    print('Calling generate_DimDate()...')\n    DimDate = generate_DimDate()\n    write_to_json_file('DimDate', DimDate)\n    print('Calling generate_DimPaymentMethod()...')\n    DimPaymentMethod = generate_DimPaymentMethod()\n    write_to_json_file('DimPaymentMethod', DimPaymentMethod)",
        "detail": "db.call_faker",
        "documentation": {}
    },
    {
        "label": "generate_code",
        "kind": 2,
        "importPath": "db.code_generator",
        "description": "db.code_generator",
        "peekOfCode": "def generate_code(table, ddl, dependencies, tables_str):\n    print(f\"before table string is : {tables_str}\")\n    tables_str = str(tables_str)\n    if tables_str and tables_str.strip() != \"\":\n      tables_str = \"Consider the following constraints for generating column data:\\n\" + tables_str\n    print(f\"after table string is : {tables_str}\")\n    \"\"\"\n    \"\"\"\n    try:\n        chain = PROMPT | llm | StrOutputParser()",
        "detail": "db.code_generator",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "db.code_generator",
        "description": "db.code_generator",
        "peekOfCode": "llm = get_llm()\noutput_parser = StrOutputParser()\nPROMPT = PromptTemplate.from_template(\"\"\"\nYou are a Python code generator specialized in creating synthetic data using the Faker library.\nYour task:\nGenerate a complete Python function named generate_{table} that produces synthetic data for \na single database table, respecting constraints and dependencies.\nInputs:\n1. ddl (string): The full CREATE TABLE SQL statement for the target table:\n{ddl}",
        "detail": "db.code_generator",
        "documentation": {}
    },
    {
        "label": "output_parser",
        "kind": 5,
        "importPath": "db.code_generator",
        "description": "db.code_generator",
        "peekOfCode": "output_parser = StrOutputParser()\nPROMPT = PromptTemplate.from_template(\"\"\"\nYou are a Python code generator specialized in creating synthetic data using the Faker library.\nYour task:\nGenerate a complete Python function named generate_{table} that produces synthetic data for \na single database table, respecting constraints and dependencies.\nInputs:\n1. ddl (string): The full CREATE TABLE SQL statement for the target table:\n{ddl}\n2. dependencies (list of dependency table names): The following are passed as lists of dicts representing",
        "detail": "db.code_generator",
        "documentation": {}
    },
    {
        "label": "PROMPT",
        "kind": 5,
        "importPath": "db.code_generator",
        "description": "db.code_generator",
        "peekOfCode": "PROMPT = PromptTemplate.from_template(\"\"\"\nYou are a Python code generator specialized in creating synthetic data using the Faker library.\nYour task:\nGenerate a complete Python function named generate_{table} that produces synthetic data for \na single database table, respecting constraints and dependencies.\nInputs:\n1. ddl (string): The full CREATE TABLE SQL statement for the target table:\n{ddl}\n2. dependencies (list of dependency table names): The following are passed as lists of dicts representing\nforeign key values: {dependencies}",
        "detail": "db.code_generator",
        "documentation": {}
    },
    {
        "label": "dt",
        "kind": 5,
        "importPath": "db.code_generator",
        "description": "db.code_generator",
        "peekOfCode": "dt = faker.date_time_this_year(before_now=True)                  \n# Format as required\nformatted = dt.strftime(\"%Y-%m-%d %H:%M:%S\") + \".000\"\nExample 1 : if in dependencies roles → [] there is no any dependency generate function structure like this\n            def generate_roles(n=5000):\nExample 2 : if in dependencies inventory → ['products', 'warehouses'] there are two dependencies then generate function structure like this\n            def generate_inventory(warehouses, products, n=5000):\nExample 3 : if in function structure there is created_by_data or updated_by_data generate function structure like this\n            def function_name(created_by_data=None, updated_by_data=None, n=5000):\nExample 4: Faker should respect schema constraints on data length.",
        "detail": "db.code_generator",
        "documentation": {}
    },
    {
        "label": "formatted",
        "kind": 5,
        "importPath": "db.code_generator",
        "description": "db.code_generator",
        "peekOfCode": "formatted = dt.strftime(\"%Y-%m-%d %H:%M:%S\") + \".000\"\nExample 1 : if in dependencies roles → [] there is no any dependency generate function structure like this\n            def generate_roles(n=5000):\nExample 2 : if in dependencies inventory → ['products', 'warehouses'] there are two dependencies then generate function structure like this\n            def generate_inventory(warehouses, products, n=5000):\nExample 3 : if in function structure there is created_by_data or updated_by_data generate function structure like this\n            def function_name(created_by_data=None, updated_by_data=None, n=5000):\nExample 4: Faker should respect schema constraints on data length.\nFor example, session_id CHAR(36) allows a maximum of 36 characters, so Faker must generate a value no longer than 36 characters\n\"\"\")",
        "detail": "db.code_generator",
        "documentation": {}
    },
    {
        "label": "generate_parent_table_data",
        "kind": 2,
        "importPath": "db.data_generator",
        "description": "db.data_generator",
        "peekOfCode": "def generate_parent_table_data(schema_path: str, table_name: str):\n    print(\"Inside generate_parent_table_data\")\n    with open(schema_path, 'r') as f:\n        sql_schema = f.read()\n    chain = prompt | llm | StrOutputParser()\n    print(f\"prompt is {prompt}\")\n    result = chain.invoke({\n        \"sql_schema\": sql_schema,\n        \"table_name\": table_name\n    })",
        "detail": "db.data_generator",
        "documentation": {}
    },
    {
        "label": "generate_child_table_data",
        "kind": 2,
        "importPath": "db.data_generator",
        "description": "db.data_generator",
        "peekOfCode": "def generate_child_table_data(schema_path: str, table_name: str, parent_table_data: Set[str]):\n    print(\"Inside generate_child_table_data\")\n    with open(schema_path, 'r') as f:\n        sql_schema = f.read()\n    chain = child_prompt | llm | StrOutputParser()\n    print(f\"child prompt is \\n {child_prompt}\")\n    result = chain.invoke({\n        \"sql_schema\": sql_schema,\n        \"table_name\": table_name,\n        \"parent_table_data\": parent_table_data",
        "detail": "db.data_generator",
        "documentation": {}
    },
    {
        "label": "temp_dir",
        "kind": 5,
        "importPath": "db.data_generator",
        "description": "db.data_generator",
        "peekOfCode": "temp_dir = tempfile.gettempdir()\nprint(f\"temp directory is {temp_dir}\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a SQL expert specialized in generating realistic INSERT statements for synthetic data.\"),\n    (\"human\", \"\"\"\nGiven the SQL schema below, extract only the DDL for the table `{table_name}`. However don't add or print this to the\nresponse.This is only for your reference.\n### SQL SCHEMA\n{sql_schema}\nThen, based on that DDL, generate exactly 5 realistic `INSERT INTO` statements for `{table_name}`.",
        "detail": "db.data_generator",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "db.data_generator",
        "description": "db.data_generator",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a SQL expert specialized in generating realistic INSERT statements for synthetic data.\"),\n    (\"human\", \"\"\"\nGiven the SQL schema below, extract only the DDL for the table `{table_name}`. However don't add or print this to the\nresponse.This is only for your reference.\n### SQL SCHEMA\n{sql_schema}\nThen, based on that DDL, generate exactly 5 realistic `INSERT INTO` statements for `{table_name}`.\nTask:\n- Respect all data types, constraints (NOT NULL, UNIQUE, etc.), and formatting in the schema.",
        "detail": "db.data_generator",
        "documentation": {}
    },
    {
        "label": "child_prompt",
        "kind": 5,
        "importPath": "db.data_generator",
        "description": "db.data_generator",
        "peekOfCode": "child_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a SQL expert that generates realistic INSERT statements for synthetic data.\"),\n    (\"human\", \"\"\"\nGiven the SQL schema below, extract only the DDL for the table `{table_name}`. However don't add or print this to the \nresponse. This is only for your reference.\n### SQL SCHEMA\n{sql_schema}\nThe table `{table_name}` has foreign key dependencies on the following parent tables. Below are the existing insert\nstatements for those parent tables. You must use the actual values from these inserts when generating foreign key values\nin the child table.",
        "detail": "db.data_generator",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "db.data_generator",
        "description": "db.data_generator",
        "peekOfCode": "llm = get_llm()\ndef generate_parent_table_data(schema_path: str, table_name: str):\n    print(\"Inside generate_parent_table_data\")\n    with open(schema_path, 'r') as f:\n        sql_schema = f.read()\n    chain = prompt | llm | StrOutputParser()\n    print(f\"prompt is {prompt}\")\n    result = chain.invoke({\n        \"sql_schema\": sql_schema,\n        \"table_name\": table_name",
        "detail": "db.data_generator",
        "documentation": {}
    },
    {
        "label": "get_ddls_for_tables",
        "kind": 2,
        "importPath": "db.ddl_info_extractor",
        "description": "db.ddl_info_extractor",
        "peekOfCode": "def get_ddls_for_tables(sql_file_path: str) -> dict:\n    \"\"\"\n    Parses a .sql file containing multiple CREATE TABLE statements and returns\n    a dictionary with table names as keys and full DDLs as values.\n    Args:\n        sql_file_path (str): Path to the SQL file.\n    Returns:\n        dict: {table_name: ddl_statement}\n    \"\"\"\n    import re",
        "detail": "db.ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "parse_sql_file",
        "kind": 2,
        "importPath": "db.ddl_info_extractor",
        "description": "db.ddl_info_extractor",
        "peekOfCode": "def parse_sql_file(file_path: str):\n    \"\"\"Parse the SQL schema file using sqlparse.\"\"\"\n    with open(file_path, 'r') as file:\n        content = file.read()\n    return sqlparse.parse(content)\n# def extract_table_dependencies(statements) -> Dict[str, Set[str]]:\n#     \"\"\"\n#     Extract tables and their foreign key dependencies.\n#     Supports both explicit and inline foreign keys.\n#     Returns a dictionary where key is the table and value is a set of referenced tables.",
        "detail": "db.ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "extract_table_dependencies",
        "kind": 2,
        "importPath": "db.ddl_info_extractor",
        "description": "db.ddl_info_extractor",
        "peekOfCode": "def extract_table_dependencies(statements) -> Dict[str, List[str]]:\n    \"\"\"\n    Extract tables and their foreign key dependencies.\n    Supports both explicit and inline foreign keys.\n    Returns a dictionary where key is the table and value is a list of referenced tables.\n    \"\"\"\n    dependencies = defaultdict(list)\n    all_tables = []\n    create_table_regex = re.compile(\n    r'CREATE TABLE\\s+(?:IF NOT EXISTS\\s+)?[`\"]?(\\w+)[`\"]?', re.IGNORECASE)",
        "detail": "db.ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "topological_sort",
        "kind": 2,
        "importPath": "db.ddl_info_extractor",
        "description": "db.ddl_info_extractor",
        "peekOfCode": "def topological_sort(graph: Dict[str, Set[str]]) -> List[str]:\n    \"\"\"Performs topological sort using DFS, ensuring parent tables appear before child tables.\"\"\"\n    visited = set()\n    visiting = set()\n    result = []\n    def dfs(node):\n        if node in visited:\n            return\n        if node in visiting:\n            raise ValueError(f\"Cyclic dependency detected at {node}\")",
        "detail": "db.ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "get_table_order",
        "kind": 2,
        "importPath": "db.ddl_info_extractor",
        "description": "db.ddl_info_extractor",
        "peekOfCode": "def get_table_order(sql_file_path: str) -> Tuple[List[str], Dict[str, Set[str]]]:\n    \"\"\"\n    Returns the ordered list of tables and the dependency graph\n    based on topological sort of foreign key dependencies.\n    \"\"\"\n    parsed_statements = parse_sql_file(sql_file_path)\n    graph = extract_table_dependencies(parsed_statements)\n    tables_sorted = topological_sort(graph)\n    print(\"✅ Table generation order:\")\n    for i, table in enumerate(tables_sorted, start=1):",
        "detail": "db.ddl_info_extractor",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "db.generate_inserts",
        "description": "db.generate_inserts",
        "peekOfCode": "def main(schema_file):\n    base_temp_dir = tempfile.gettempdir()  # e.g., /var/folders/... on macOS\n    full_output_dir = os.path.join(base_temp_dir, \"output\")\n    if os.path.exists(full_output_dir) and os.path.isdir(full_output_dir):\n        shutil.rmtree(full_output_dir)\n    main_faker()\n    ddls = get_ddls_for_tables(schema_file)\n    sorted_tables = get_table_order(schema_file)\n    base_temp_dir = tempfile.gettempdir()\n    full_output_dir = os.path.join(base_temp_dir, \"output\")",
        "detail": "db.generate_inserts",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "db.main",
        "description": "db.main",
        "peekOfCode": "def main(schema_file):\n    ddls = get_ddls_for_tables(schema_file)\n    sorted_tables, dependency_graphs = get_table_order(schema_file)\n    create_insertion_data_methods(sorted_tables, dependency_graphs, ddls)\n    generate_caller_script_with_vars(dependency_graphs, sorted_tables)\nif __name__ == \"__main__\":\n    start = datetime.now()\n    print(f\"start time is {start}.\")\n    load_dotenv()\n    schema_file_path = os.getenv(\"DB_SCHEMA_FILE\")",
        "detail": "db.main",
        "documentation": {}
    },
    {
        "label": "load_schema_info",
        "kind": 2,
        "importPath": "db.process_tables",
        "description": "db.process_tables",
        "peekOfCode": "def load_schema_info():\n    load_dotenv()\n    schema_path = os.getenv(\"DB_SCHEMA_INFO\")\n    if not schema_path or not os.path.isfile(schema_path):\n        raise FileNotFoundError(f\"Schema file not found or DB_SCHEMA_INFO not set: {schema_path}\")\n    with open(schema_path, 'r', encoding='utf-8') as file:\n        content = file.read()\n        if not content.strip():\n            raise ValueError(f\"Schema file is empty: {schema_path}\")\n        return json.loads(content)",
        "detail": "db.process_tables",
        "documentation": {}
    },
    {
        "label": "remove_synthetic_sql_files",
        "kind": 2,
        "importPath": "db.process_tables",
        "description": "db.process_tables",
        "peekOfCode": "def remove_synthetic_sql_files():\n    temp_dir = tempfile.gettempdir()\n    for filename in os.listdir(temp_dir):\n        if filename.endswith('_synthetic_data.sql'):\n            os.remove(os.path.join(temp_dir, filename))\ndef merge_sql_files(file_list, output_file):\n    with open(output_file, 'w') as outfile:\n        for file in file_list:\n            file_path = os.path.join(tempfile.gettempdir(), f\"{file}_synthetic_data.sql\")\n            with open(file_path, 'r') as infile:",
        "detail": "db.process_tables",
        "documentation": {}
    },
    {
        "label": "merge_sql_files",
        "kind": 2,
        "importPath": "db.process_tables",
        "description": "db.process_tables",
        "peekOfCode": "def merge_sql_files(file_list, output_file):\n    with open(output_file, 'w') as outfile:\n        for file in file_list:\n            file_path = os.path.join(tempfile.gettempdir(), f\"{file}_synthetic_data.sql\")\n            with open(file_path, 'r') as infile:\n                outfile.write(infile.read())\ndef get_parent_data_inserts(tables: Set[str]) -> Set[str]:\n    temp_dir = tempfile.gettempdir()\n    result_set = set()\n    for table in tables:",
        "detail": "db.process_tables",
        "documentation": {}
    },
    {
        "label": "get_parent_data_inserts",
        "kind": 2,
        "importPath": "db.process_tables",
        "description": "db.process_tables",
        "peekOfCode": "def get_parent_data_inserts(tables: Set[str]) -> Set[str]:\n    temp_dir = tempfile.gettempdir()\n    result_set = set()\n    for table in tables:\n        file_path = os.path.join(temp_dir, f\"{table}_synthetic_data.sql\")\n        if os.path.exists(file_path):\n            with open(file_path, 'r') as file:\n                content = file.read().strip()\n                result_set.add(f\"# {table}\\n{content}\")\n        else:",
        "detail": "db.process_tables",
        "documentation": {}
    },
    {
        "label": "process_generated_data",
        "kind": 2,
        "importPath": "db.process_tables",
        "description": "db.process_tables",
        "peekOfCode": "def process_generated_data(sorted_tables, dependency_graphs, schema_file_path):\n    for idx, table in enumerate(sorted_tables, 1):\n        dependencies = dependency_graphs.get(table, set())\n        if not dependencies:\n            print(f\"{idx}. Processing table: {table}\")\n            data_generator.generate_parent_table_data(schema_file_path, table)\n        else:\n            print(f\"{idx}. Processing {table}, has dependencies: {dependencies}\")\n            inserts = get_parent_data_inserts(dependencies)\n            data_generator.generate_child_table_data(schema_file_path, table, inserts)",
        "detail": "db.process_tables",
        "documentation": {}
    },
    {
        "label": "create_insertion_data_methods",
        "kind": 2,
        "importPath": "db.process_tables",
        "description": "db.process_tables",
        "peekOfCode": "def create_insertion_data_methods(sorted_tables, dependency_graphs, ddl_dict):\n    print(\"\\n Inside process_generated_data\")\n    folder_name = \"do\"\n    if os.path.exists(folder_name):\n        shutil.rmtree(folder_name)\n    os.makedirs(folder_name, exist_ok=True)\n    filepath=\"dependency_graph.txt\"\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        for key, deps in dependency_graphs.items():\n            f.write(f\"{key} → {deps}\\n\")",
        "detail": "db.process_tables",
        "documentation": {}
    }
]